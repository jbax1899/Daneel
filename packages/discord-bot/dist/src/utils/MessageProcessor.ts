import fs from 'fs';
import * as path from 'path';
import { Message } from 'discord.js';
import { OpenAIService, SupportedModel, TTSOptions } from './openaiService.js';
import { logger } from './logger.js';
import { ResponseHandler } from './response/ResponseHandler.js';
import { RateLimiter } from './RateLimiter.js';
import { config } from './env.js';
import { Planner, Plan } from './prompting/Planner.js';
import { TTS_DEFAULT_OPTIONS } from './openaiService.js';
//import { Pinecone } from '@pinecone-database/pinecone';
import { ContextBuilder } from './prompting/ContextBuilder.js';

type MessageProcessorOptions = {
  openaiService: OpenAIService;
  planner?: Planner;
  systemPrompt?: string;
};

const MAIN_MODEL: SupportedModel = 'gpt-5-mini';
const PLAN_CONTEXT_SIZE = 8;
const RESPONSE_CONTEXT_SIZE = 24;

export class MessageProcessor {
  private readonly openaiService: OpenAIService;
  private readonly contextBuilder: ContextBuilder;
  private readonly planner: Planner;
  private readonly rateLimiters: { user?: RateLimiter; channel?: RateLimiter; guild?: RateLimiter };
  //private readonly pineconeClient = new Pinecone({ apiKey: process.env.PINECONE_API_KEY || '' });
  //private readonly repoIndex = this.pineconeClient.index('discord-bot-code', 'discord-bot-code-v3tu03c.svc.aped-4627-b74a.pinecone.io');

  constructor(options: MessageProcessorOptions) {
    this.openaiService = options.openaiService;
    this.contextBuilder = new ContextBuilder(this.openaiService);
    this.planner = options.planner ?? new Planner(this.openaiService);

    this.rateLimiters = {};
    if (config.rateLimits.user.enabled)     this.rateLimiters.user    = new RateLimiter({ limit: config.rateLimits.user.limit,    window: config.rateLimits.user.windowMs,    scope: 'user' });
    if (config.rateLimits.channel.enabled)  this.rateLimiters.channel = new RateLimiter({ limit: config.rateLimits.channel.limit, window: config.rateLimits.channel.windowMs, scope: 'channel' });
    if (config.rateLimits.guild.enabled)    this.rateLimiters.guild   = new RateLimiter({ limit: config.rateLimits.guild.limit,   window: config.rateLimits.guild.windowMs,   scope: 'guild' });
  }

  /**
   * 
   * Processes a message and generates a response based on the plan generated by the planner.
   * @param {Message} message - The message to process
   */
  public async processMessage(message: Message, directReply: boolean = true, trigger: string = ''): Promise<void> {
    const responseHandler = new ResponseHandler(message, message.channel, message.author);

    if (!message.content.trim()) return; // Ignore empty messages

    //logger.debug(`Processing message from ${message.author.id}/${message.author.tag}: ${message.content.slice(0, 100)}...`);

    // Rate limit check
    const rateLimitResult = await this.checkRateLimits(message);
    if (!rateLimitResult.allowed && rateLimitResult.error) {
      await responseHandler.sendMessage(rateLimitResult.error);
      return;
    }

    // Build context for plan
    const { context: planContext } = await this.contextBuilder.buildMessageContext(message, PLAN_CONTEXT_SIZE);

    // If there are images attached to the trigger message, process them
    let imageDescriptions: string[] = [];
    let flatImageDescriptions: string = '';
    const imageAttachments = message.attachments.filter(a => a.contentType?.startsWith('image/'));

    if (imageAttachments.size > 0) {
      logger.debug(`Processing image attachment from ${message.author.id}/${message.author.tag}`);

      // Generate descriptions for all images
      imageDescriptions.push(...await Promise.all(
        imageAttachments.map(async (a) => {
          try {
            const resp = await this.openaiService.generateImageDescription(a.url, message.content);
            return resp.message?.content ?? `Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}`;
          } catch (error) {
            logger.error(`Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}: ${error}`);
            return `Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}`;
          }
        })
      ));

      // Add the image descriptions to the plan context
      flatImageDescriptions = imageDescriptions
        .map((desc, index) => `[Image ${index + 1}]: ${desc}`)
        .join('\n');

      planContext.push({
        role: 'system',
        content: `User also uploaded images with these automatically generated descriptions: ${flatImageDescriptions}`
      });
    }

    // Generate plan
    const plan: Plan = await this.planner.generatePlan(planContext, trigger);

    // If the plan updated the bot's presence, update it
    if (plan.presence) {
      logger.debug(`Updating presence: ${JSON.stringify(plan.presence)}`);

      // Verify presence options
      let verifiedPresenceOptions = {
        status: plan.presence.status,
        activities: plan.presence.activities,
        shardId: plan.presence.shardId,
        afk: plan.presence.afk
      }

      responseHandler.setPresence(verifiedPresenceOptions);
    }

    //
    // Handle response based on plan
    //
    switch (plan.action) {
      //
      // Ignore
      //
      case 'ignore':
        logger.debug(`Ignoring message: ${message.content.slice(0, 100)}...`);
        return;
      //
      // Regular message response
      //
      case 'message':
        logger.debug(`Generating response for message: ${message.content.slice(0, 100)}...`);

        await responseHandler.startTyping(); // Start persistent typing indicator

        // TODO: Instead of a fixed context size, use the plan's tool call to suggest which messages to include
        let { context: responseContext } = await this.contextBuilder.buildMessageContext(message, RESPONSE_CONTEXT_SIZE);

        // Add image descriptions to context, if any
        if (flatImageDescriptions) {
          responseContext.push({
            role: 'system',
            content: `User also uploaded images with these automatically generated descriptions: 
            ${flatImageDescriptions}
            Pass through these descriptions exactly as recieved, with a header like 'Descriptions of attached image(s):' and with each description prefixed like '[Image #]: ', and placed all within a code block (\`\`\`example\`\`\`) at the end of your response.`
          });
        }

        // If the plan requested information about the repository, retrieve it
        // Disabled for now
        /*
        if (plan.repoQuery) {
          const queryTexts = plan.repoQuery
            .split(',')
            .map(q => q.trim())
            .filter(Boolean); // remove empty strings
        
          await Promise.all(
            queryTexts.map(async q => {
              // 1. Generate embedding for this query
              const embedding1024 = await this.openaiService.embedText(q, 1024);
        
              // 2. Query Pinecone
              const results = await this.repoIndex.query({
                vector: embedding1024,
                topK: 10,
                includeMetadata: true
              });

              // Keep only TS or MD files
              const filtered = results.matches.filter(
                m => (m.metadata?.filePath as string)?.endsWith('.ts') || (m.metadata?.filePath as string)?.endsWith('.md')
              );
        
              logger.debug(`Retrieved repository information for query "${q}" (not an exhaustive list): ${JSON.stringify(filtered)}`);
              
              // 3. Flatten results and add to context as a system message
              const message: OpenAIMessage = { "role": "system", "content": "Repository information relevant to query \"${q}\":\n${filtered.map(r => JSON.stringify(r.metadata)).join('\n')}" };
              trimmedContext.push(message);
            })
          );
        }
        */

        try {
          // Generate AI response
          logger.debug(`Generating AI response with options: ${JSON.stringify(plan.openaiOptions)}`);
          const aiResponse = await this.openaiService.generateResponse(
            MAIN_MODEL,
            responseContext,
            plan.openaiOptions
          );
          logger.debug(`Response recieved. Usage: ${JSON.stringify(aiResponse.usage)}`);

          // Get the assistant's response
          const responseText = aiResponse.message?.content || 'No response generated.';

          // If the response is to be read out loud, generate speech (TTS)
          let ttsPath: string | null = null;
          if (plan.modality === 'tts') {
            // Use plan's TTS options if they exist, otherwise fall back to defaults
            const ttsOptions: TTSOptions = plan.openaiOptions?.ttsOptions || TTS_DEFAULT_OPTIONS;

            // Generate speech
            ttsPath = await this.openaiService.generateSpeech(
              responseText,
              ttsOptions,
              Date.now().toString(),
              'mp3'
            );
          }

          // If the assistant has a response, send it
          if (responseText) {
            // Special rules if we're sending a TTS
            if (ttsPath) {
              // Read the file into a Buffer
              const fileBuffer = await fs.promises.readFile(ttsPath);

              // If we're sending a TTS too, put the text transcript in a code block
              const cleanResponseText = responseText.replace(/\n/g, ' ').replace(/`/g, ''); // Replace newlines with spaces, remove backticks (since we are putting it in a code block)

              // Send the response
              await responseHandler.sendMessage(
                `\`\`\`${cleanResponseText}\`\`\``, // markdown code block for transcript
                [{
                  filename: path.basename(ttsPath),
                  data: fileBuffer
                }],
                true // Make it a reply
              );
              // delete the tts file
              fs.unlinkSync(ttsPath);
            } else {
              // Not tts, send regular response
              await responseHandler.sendMessage(responseText, [], directReply);
            }
            logger.debug(`Response sent (${responseText}) for message: ${message.content.slice(0, 100)}...`);
          }
        } finally {
          responseHandler.stopTyping(); // Stop typing indicator
        }
        return;
      //
      // React with emoji (one or more) using Discord's built-in reaction feature
      //
      case 'react':
        if (plan.reaction) {
          await responseHandler.addReaction(plan.reaction);
          logger.debug(`Reaction(s) sent (${plan.reaction}) for message: ${message.content.slice(0, 100)}...`);
        } else {
          logger.debug(`No reaction specified. Ignoring message: ${message.content.slice(0, 100)}...`);
        }
        return;
      //
      // Unspecified action: Ignore
      //
      default:
        logger.debug(`No action specified. Ignoring message: ${message.content.slice(0, 100)}...`);
        return;
    }
  }

  private async checkRateLimits(message: Message): Promise<{ allowed: boolean; error?: string }> {
    const results: Array<{ allowed: boolean; error?: string }> = [];

    if (this.rateLimiters.user) results.push(await this.rateLimiters.user.check(message.author.id, message.channel.id, message.guild?.id));
    if (this.rateLimiters.channel) results.push(await this.rateLimiters.channel.check(message.author.id, message.channel.id, message.guild?.id));
    if (this.rateLimiters.guild && message.guild) results.push(await this.rateLimiters.guild.check(message.author.id, message.channel.id, message.guild.id));

    return results.find(r => !r.allowed) ?? { allowed: true };
  }
}