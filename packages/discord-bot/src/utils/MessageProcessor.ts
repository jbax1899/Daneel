import fs from 'fs';
import * as path from 'path';
import { Message } from 'discord.js';
import { OpenAIService, SupportedModel, TTSOptions } from './openaiService.js';
import { logger } from './logger.js';
import { ResponseHandler } from './response/ResponseHandler.js';
import { RateLimiter, imageCommandRateLimiter } from './RateLimiter.js';
import { config } from './env.js';
import { Planner, Plan } from './prompting/Planner.js';
import { TTS_DEFAULT_OPTIONS } from './openaiService.js';
//import { Pinecone } from '@pinecone-database/pinecone';
import { ContextBuilder } from './prompting/ContextBuilder.js';
import { DEFAULT_MODEL } from '../commands/image/constants.js';
import {
  buildImageResponseContent,
  createRetryButtonRow,
  createVariationButtonRow,
  executeImageGeneration,
  formatRetryCountdown
} from '../commands/image/sessionHelpers.js';
import { saveFollowUpContext, type ImageGenerationContext } from '../commands/image/followUpCache.js';
import type { ImageBackgroundType, ImageQualityType, ImageSizeType, ImageStylePreset } from '../commands/image/types.js';

type MessageProcessorOptions = {
  openaiService: OpenAIService;
  planner?: Planner;
  systemPrompt?: string;
};

const MAIN_MODEL: SupportedModel = 'gpt-5-mini';
const PLAN_CONTEXT_SIZE = 8;
const RESPONSE_CONTEXT_SIZE = 24;
const VALID_IMAGE_BACKGROUNDS: ImageBackgroundType[] = ['auto', 'transparent', 'opaque'];
const VALID_IMAGE_STYLES = new Set<ImageStylePreset>([
  'natural',
  'vivid',
  'photorealistic',
  'cinematic',
  'oil_painting',
  'watercolor',
  'digital_painting',
  'line_art',
  'sketch',
  'cartoon',
  'anime',
  'comic',
  'pixel_art',
  'cyberpunk',
  'fantasy_art',
  'surrealist',
  'minimalist',
  'vintage',
  'noir',
  '3d_render',
  'steampunk',
  'abstract',
  'pop_art',
  'dreamcore',
  'isometric',
  'unspecified'
]);

export class MessageProcessor {
  private readonly openaiService: OpenAIService;
  private readonly contextBuilder: ContextBuilder;
  private readonly planner: Planner;
  private readonly rateLimiters: { user?: RateLimiter; channel?: RateLimiter; guild?: RateLimiter };
  //private readonly pineconeClient = new Pinecone({ apiKey: process.env.PINECONE_API_KEY || '' });
  //private readonly repoIndex = this.pineconeClient.index('discord-bot-code', 'discord-bot-code-v3tu03c.svc.aped-4627-b74a.pinecone.io');

  constructor(options: MessageProcessorOptions) {
    this.openaiService = options.openaiService;
    this.contextBuilder = new ContextBuilder(this.openaiService);
    this.planner = options.planner ?? new Planner(this.openaiService);

    this.rateLimiters = {};
    if (config.rateLimits.user.enabled)     this.rateLimiters.user    = new RateLimiter({ limit: config.rateLimits.user.limit,    window: config.rateLimits.user.windowMs,    scope: 'user' });
    if (config.rateLimits.channel.enabled)  this.rateLimiters.channel = new RateLimiter({ limit: config.rateLimits.channel.limit, window: config.rateLimits.channel.windowMs, scope: 'channel' });
    if (config.rateLimits.guild.enabled)    this.rateLimiters.guild   = new RateLimiter({ limit: config.rateLimits.guild.limit,   window: config.rateLimits.guild.windowMs,   scope: 'guild' });
  }

  /**
   * 
   * Processes a message and generates a response based on the plan generated by the planner.
   * @param {Message} message - The message to process
   */
  public async processMessage(message: Message, directReply: boolean = true, trigger: string = ''): Promise<void> {
    const responseHandler = new ResponseHandler(message, message.channel, message.author);

    if (!message.content.trim()) return; // Ignore empty messages

    //logger.debug(`Processing message from ${message.author.id}/${message.author.tag}: ${message.content.slice(0, 100)}...`);

    // Rate limit check
    const rateLimitResult = await this.checkRateLimits(message);
    if (!rateLimitResult.allowed && rateLimitResult.error) {
      await responseHandler.sendMessage(rateLimitResult.error);
      return;
    }

    // Build context for plan
    const { context: planContext } = await this.contextBuilder.buildMessageContext(message, PLAN_CONTEXT_SIZE);

    // If there are images attached to the trigger message, process them
    let imageDescriptions: string[] = [];
    let flatImageDescriptions: string = '';
    const imageAttachments = message.attachments.filter(a => a.contentType?.startsWith('image/'));

    if (imageAttachments.size > 0) {
      logger.debug(`Processing image attachment from ${message.author.id}/${message.author.tag}`);

      // Generate descriptions for all images
      imageDescriptions.push(...await Promise.all(
        imageAttachments.map(async (a) => {
          try {
            const resp = await this.openaiService.generateImageDescription(a.url, message.content);
            return resp.message?.content ?? `Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}`;
          } catch (error) {
            logger.error(`Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}: ${error}`);
            return `Error generating image description for ${message.author.id}/${message.author.tag} with image ${a.url}`;
          }
        })
      ));

      // Add the image descriptions to the plan context
      flatImageDescriptions = imageDescriptions
        .map((desc, index) => `[Image ${index + 1}]: ${desc}`)
        .join('\n');

      planContext.push({
        role: 'system',
        content: `User also uploaded images with these automatically generated descriptions: ${flatImageDescriptions}`
      });
    }

    // Generate plan
    const plan: Plan = await this.planner.generatePlan(planContext, trigger);

    // If the plan updated the bot's presence, update it
    if (plan.presence) {
      logger.debug(`Updating presence: ${JSON.stringify(plan.presence)}`);

      // Verify presence options
      let verifiedPresenceOptions = {
        status: plan.presence.status,
        activities: plan.presence.activities,
        shardId: plan.presence.shardId,
        afk: plan.presence.afk
      }

      responseHandler.setPresence(verifiedPresenceOptions);
    }

    //
    // Handle response based on plan
    //
    switch (plan.action) {
      //
      // Ignore
      //
      case 'ignore':
        logger.debug(`Ignoring message: ${message.content.slice(0, 100)}...`);
        return;
      //
      // Regular message response
      //
      case 'image': {
        logger.debug(`Planner requested automated image generation for message: ${message.content.slice(0, 100)}...`);

        const request = plan.imageRequest;
        if (!request?.prompt?.trim()) {
          logger.warn('Image plan was missing a prompt; falling back to ignoring the request.');
          return;
        }

        const trimmedPrompt = request.prompt.trim();
        const requestedAspect = (request.aspectRatio ?? 'auto').toLowerCase() as ImageGenerationContext['aspectRatio'];

        // Translate the planner's aspect ratio preference into concrete size settings.
        let size: ImageSizeType = 'auto';
        let aspectRatio: ImageGenerationContext['aspectRatio'] = 'auto';
        let aspectRatioLabel: ImageGenerationContext['aspectRatioLabel'] = 'Auto';

        switch (requestedAspect) {
          case 'square':
            size = '1024x1024';
            aspectRatio = 'square';
            aspectRatioLabel = 'Square';
            break;
          case 'portrait':
            size = '1024x1536';
            aspectRatio = 'portrait';
            aspectRatioLabel = 'Portrait';
            break;
          case 'landscape':
            size = '1536x1024';
            aspectRatio = 'landscape';
            aspectRatioLabel = 'Landscape';
            break;
          default:
            aspectRatio = 'auto';
            aspectRatioLabel = 'Auto';
        }

        const requestedBackground = request.background?.toLowerCase() ?? 'auto';
        const background = VALID_IMAGE_BACKGROUNDS.includes(requestedBackground as ImageBackgroundType)
          ? requestedBackground as ImageBackgroundType
          : 'auto';

        const normalizedStyle = request.style
          ? request.style.toLowerCase().replace(/[^a-z0-9]+/g, '_')
          : 'unspecified';
        const style = VALID_IMAGE_STYLES.has(normalizedStyle as ImageStylePreset)
          ? normalizedStyle as ImageStylePreset
          : 'unspecified';

        // Assemble the same context structure used by the slash command pipeline so follow-ups work identically.
        const context: ImageGenerationContext = {
          prompt: trimmedPrompt,
          model: DEFAULT_MODEL,
          size,
          aspectRatio,
          aspectRatioLabel,
          quality: 'low' as ImageQualityType,
          qualityRestricted: false,
          background,
          style,
          allowPromptAdjustment: request.allowPromptAdjustment ?? true,
          authorUserId: message.author.id,
          authorGuildId: message.guild?.id ?? null
        };

        const isDeveloper = message.author.id === process.env.DEVELOPER_USER_ID;
        if (!isDeveloper) {
          // Respect the existing image cooldown before starting work. If the user is on cooldown, stash the
          // context so the retry button can pick up exactly where we left off once the timer expires.
          const { allowed, retryAfter, error } = imageCommandRateLimiter.checkRateLimitImageCommand(message.author.id);
          if (!allowed) {
            const countdown = formatRetryCountdown(retryAfter ?? 0);
            const retryKey = `retry:${message.id}`;
            saveFollowUpContext(retryKey, context);
            const retryRow = createRetryButtonRow(retryKey, countdown);
            await responseHandler.sendMessage(
              `⚠️ ${error ?? 'I need a moment before I can create another image.'} Try again in ${countdown}.`,
              [],
              directReply,
              false,
              [retryRow]
            );
            return;
          }
        }

        await responseHandler.startTyping();

        try {
          // Reuse the shared generation helper so we get identical cost calculations and Cloudinary uploads.
          const artifacts = await executeImageGeneration(context, {
            user: {
              username: message.author.username,
              nickname: message.member?.displayName ?? message.author.username,
              guildName: message.guild?.name ?? 'Direct message channel'
            }
          });

          // Store the new response ID for future variation requests.
          if (artifacts.responseId) {
            saveFollowUpContext(artifacts.responseId, context);
          }

          const components = artifacts.responseId ? [createVariationButtonRow(artifacts.responseId)] : [];
          const files = [{ filename: artifacts.finalImageFileName, data: artifacts.finalImageBuffer }];
          const content = buildImageResponseContent(context, artifacts);

          await responseHandler.sendMessage(content, files, directReply, false, components);
          logger.debug(`Automated image response sent for message: ${message.id}`);
        } catch (error) {
          logger.error('Automated image generation failed:', error);
          await responseHandler.sendMessage('⚠️ I tried to create an image but something went wrong.', [], directReply);
        } finally {
          // Always stop the typing indicator so the channel UI doesn't get stuck.
          responseHandler.stopTyping();
        }

        return;
      }

      case 'message':
        logger.debug(`Generating response for message: ${message.content.slice(0, 100)}...`);

        await responseHandler.startTyping(); // Start persistent typing indicator

        // TODO: Instead of a fixed context size, use the plan's tool call to suggest which messages to include
        let { context: responseContext } = await this.contextBuilder.buildMessageContext(message, RESPONSE_CONTEXT_SIZE);

        // Add image descriptions to context, if any
        if (flatImageDescriptions) {
          responseContext.push({
            role: 'system',
            content: `User also uploaded images with these automatically generated descriptions: 
            ${flatImageDescriptions}
            Pass through these descriptions exactly as recieved, with a header like 'Descriptions of attached image(s):' and with each description prefixed like '[Image #]: ', and placed all within a code block (\`\`\`example\`\`\`) at the end of your response.`
          });
        }

        // If the plan requested information about the repository, retrieve it
        // Disabled for now
        /*
        if (plan.repoQuery) {
          const queryTexts = plan.repoQuery
            .split(',')
            .map(q => q.trim())
            .filter(Boolean); // remove empty strings
        
          await Promise.all(
            queryTexts.map(async q => {
              // 1. Generate embedding for this query
              const embedding1024 = await this.openaiService.embedText(q, 1024);
        
              // 2. Query Pinecone
              const results = await this.repoIndex.query({
                vector: embedding1024,
                topK: 10,
                includeMetadata: true
              });

              // Keep only TS or MD files
              const filtered = results.matches.filter(
                m => (m.metadata?.filePath as string)?.endsWith('.ts') || (m.metadata?.filePath as string)?.endsWith('.md')
              );
        
              logger.debug(`Retrieved repository information for query "${q}" (not an exhaustive list): ${JSON.stringify(filtered)}`);
              
              // 3. Flatten results and add to context as a system message
              const message: OpenAIMessage = { "role": "system", "content": "Repository information relevant to query \"${q}\":\n${filtered.map(r => JSON.stringify(r.metadata)).join('\n')}" };
              trimmedContext.push(message);
            })
          );
        }
        */

        try {
          // Generate AI response
          logger.debug(`Generating AI response with options: ${JSON.stringify(plan.openaiOptions)}`);
          const aiResponse = await this.openaiService.generateResponse(
            MAIN_MODEL,
            responseContext,
            plan.openaiOptions
          );
          logger.debug(`Response recieved. Usage: ${JSON.stringify(aiResponse.usage)}`);

          // Get the assistant's response
          const responseText = aiResponse.message?.content || 'No response generated.';

          // If the response is to be read out loud, generate speech (TTS)
          let ttsPath: string | null = null;
          if (plan.modality === 'tts') {
            // Use plan's TTS options if they exist, otherwise fall back to defaults
            const ttsOptions: TTSOptions = plan.openaiOptions?.ttsOptions || TTS_DEFAULT_OPTIONS;

            // Generate speech
            ttsPath = await this.openaiService.generateSpeech(
              responseText,
              ttsOptions,
              Date.now().toString(),
              'mp3'
            );
          }

          // If the assistant has a response, send it
          if (responseText) {
            // Special rules if we're sending a TTS
            if (ttsPath) {
              // Read the file into a Buffer
              const fileBuffer = await fs.promises.readFile(ttsPath);

              // If we're sending a TTS too, put the text transcript in a code block
              const cleanResponseText = responseText.replace(/\n/g, ' ').replace(/`/g, ''); // Replace newlines with spaces, remove backticks (since we are putting it in a code block)

              // Send the response
              await responseHandler.sendMessage(
                `\`\`\`${cleanResponseText}\`\`\``, // markdown code block for transcript
                [{
                  filename: path.basename(ttsPath),
                  data: fileBuffer
                }],
                true // Make it a reply
              );
              await cleanupTTSFile(ttsPath);
            } else {
              // Not tts, send regular response
              await responseHandler.sendMessage(responseText, [], directReply);
            }
            logger.debug(`Response sent (${responseText}) for message: ${message.content.slice(0, 100)}...`);
          }
        } finally {
          responseHandler.stopTyping(); // Stop typing indicator
        }
        return;
      //
      // React with emoji (one or more) using Discord's built-in reaction feature
      //
      case 'react':
        if (plan.reaction) {
          await responseHandler.addReaction(plan.reaction);
          logger.debug(`Reaction(s) sent (${plan.reaction}) for message: ${message.content.slice(0, 100)}...`);
        } else {
          logger.debug(`No reaction specified. Ignoring message: ${message.content.slice(0, 100)}...`);
        }
        return;
      //
      // Unspecified action: Ignore
      //
      default:
        logger.debug(`No action specified. Ignoring message: ${message.content.slice(0, 100)}...`);
        return;
    }
  }

  private async checkRateLimits(message: Message): Promise<{ allowed: boolean; error?: string }> {
    const results: Array<{ allowed: boolean; error?: string }> = [];

    if (this.rateLimiters.user) results.push(await this.rateLimiters.user.check(message.author.id, message.channel.id, message.guild?.id));
    if (this.rateLimiters.channel) results.push(await this.rateLimiters.channel.check(message.author.id, message.channel.id, message.guild?.id));
    if (this.rateLimiters.guild && message.guild) results.push(await this.rateLimiters.guild.check(message.author.id, message.channel.id, message.guild.id));

    return results.find(r => !r.allowed) ?? { allowed: true };
  }
}

export async function cleanupTTSFile(ttsPath: string): Promise<void> {
  if (!ttsPath) return;

  try {
    await fs.promises.unlink(ttsPath);
  } catch (error) {
    const err = error as NodeJS.ErrnoException;
    if (err?.code === 'ENOENT') {
      return;
    }

    logger.debug(`Failed to delete TTS file ${ttsPath}: ${err?.message ?? err}`);
  }
}
